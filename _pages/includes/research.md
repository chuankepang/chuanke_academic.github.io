
# ðŸ”­ Research Experiences


<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/RSPrompter.png"><img src='image/RSPrompter.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<b>RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation based on Visual Foundation Model</b><br>
<i>Arxiv, 2023</i><br>
<b>Keyan Chen</b>, Chenyang Liu, Hao Chen, Haotian Zhang, Wenyuan Li, Zhengxia Zou, and Zhenwei Shi<br>
[<a href="https://arxiv.org/abs/2306.16269">Arxiv</a>] [<a href="https://github.com/KyanChen/RSPrompter">Github</a>] [<a href="https://kyanchen.github.io/RSPrompter/">Page</a>] [<a href="https://huggingface.co/spaces/KyanChen/RSPrompter">Demo</a>]<br>
<div style="text-align: justify">
We consider designing an automated instance segmentation approach for remote sensing images based on the SAM foundation model, incorporating semantic category information with prompt learning. 
</div>
</div>

</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/FunSR.png"><img src='image/FunSR.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<b>Continuous Remote Sensing Image Super-Resolution based on Context Interaction in Implicit Function Space</b><br>
<i>IEEE Transactions on Geoscience and Remote Sensing (TGRS), 2023</i><br>
<b>Keyan Chen</b>, Wenyuan Li, Sen Lei, Jianqi Chen, Xiaolong Jiang, Zhengxia Zou, and Zhenwei Shi<br>
[<a href="https://arxiv.org/abs/2302.08046">Arxiv</a>] [<a href="https://github.com/KyanChen/FunSR">Github</a>] [<a href="https://kyanchen.github.io/FunSR/">Page</a>] [<a href="https://huggingface.co/spaces/KyanChen/FunSR">Demo</a>]<br>
<div style="text-align: justify">
We propose a new super-resolution framework based on context interaction in implicit function space for learning continuous representations of remote sensing images, called FunSR, which consists of three main components: a functional representor, a functional interactor, and a functional parser.
</div>
</div>

</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/OvarNet.png"><img src='image/OvarNet.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<b>OvarNet: Towards Open-vocabulary Object Attribute Recognition</b><br>
<i>Computer Vision and Pattern Recognition Conference (CVPR), 2023</i><br>
<b>Keyan Chen</b>, Xiaolong Jiang, Yao Hu, Xu Tang, Yan Gao, Jianqi Chen and Weidi Xie<br>
[<a href="https://arxiv.org/abs/2301.09506">Arxiv</a>] [<a href="https://github.com/KyanChen/OvarNet">Github</a>] [<a href="https://kyanchen.github.io/OvarNet/">Page</a>]<br>
<div style="text-align: justify">
We consider the problem of simultaneously detecting objects and inferring their visual attributes in an image, even for those with no manual annotations provided at the training stage, resembling an open-vocabulary scenario.
</div>
</div>

</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/ODSurvey.png"><img src='image/ODSurvey.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<b>Object Detection in 20 Years: A Survey</b><br>
<i>Proceedings of the IEEE (P IEEE), 2023</i><br>
Zhengxia Zou, <b>Keyan Chen</b>, Zhenwei Shi, Yuhong Guo and Jieping Ye<br>
[<a href="http://levir.buaa.edu.cn/publications/od_survey.pdf">PDF</a>] [<a href="https://github.com/KyanChen/ODSurvey">Github</a>] [<a href="https://kyanchen.github.io/ODSurvey/">Page</a>]<br>
<div style="text-align: justify">
This paper extensively reviews the fast-moving research field in the light of technical evolution, spanning over a quarter-century's time (from the 1990s to 2022). A number of topics have been covered, including the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speed-up techniques, and the recent state-of-the-art detection methods.
</div>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/RASNet.png"><img src='image/RASNet.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<b>Resolution-agnostic Remote Sensing Scene Classification with Implicit Neural Representations</b><br>
<i>IEEE Geoscience and Remote Sensing Letters (GRSL), 2022</i><br>
<b>Keyan Chen</b>, Wenyuan Li, Jianqi Chen, Zhengxia Zou and Zhenwei Shi<br>
[<a href="http://levir.buaa.edu.cn/publications/RASNet.pdf">PDF</a>] [<a href="https://github.com/KyanChen/RASNet">Github</a>] [<a href="https://kyanchen.github.io/RASNet/">Page</a>]<br>
<div style="text-align: justify">
We propose a novel scene classification method with scale and resolution adaptation ability. Unlike previous CNNbased methods that make predictions based on rasterized image inputs, the proposed method converts the images as continuous functions with INRs optimization and then performs classification within the function space.
</div>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/GeCo.png"><img src='image/GeCo.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<b>Geographical Supervision Correction for Remote Sensing Representation Learning</b><br>
<i>IEEE Transactions on Geoscience and Remote Sensing (TGRS), 2022</i><br>
Wenyuan Li, <b>Keyan Chen</b>, and Zhenwei Shi<br>
[<a href="http://levir.buaa.edu.cn/publications/FINAL_VERSION.pdf">PDF</a>]<br>
<div style="text-align: justify">
We propose a Geographical supervision Correction method (GeCo) for remote sensing representation learning. Deviated geographical supervision generated by GLC products can be corrected adaptively using the correction matrix during network pre-training and joint optimization process is designed to simultaneously update the correction matrix and network parameters.
</div>

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/DRENet.png"><img src='image/DRENet.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<b>A Degraded Reconstruction Enhancement-based Method for Tiny Ship Detection in Remote Sensing Images with A New Large-scale Dataset</b><br>
<i>IEEE Transactions on Geoscience and Remote Sensing (TGRS), 2022</i><br>
Jianqi Chen, <b>Keyan Chen</b>, Hao Chen, Zhengxia Zou and Zhenwei Shi<br>
[<a href="http://levir.buaa.edu.cn/publications/DRENet.pdf">PDF</a>] [<a href="https://github.com/WindVChen/DRENet">Github</a>] [<a href="https://github.com/windvchen/levir-ship">Dataset</a>]<br>
<div style="text-align: justify">
We propose a tiny ship detection method namely, Degraded Reconstruction Enhancement Network (DRENet), for medium-resolution remote sensing images, and introduce Levir-Ship, which contains 3876 GF-1/GF-6 multi-spectral images and over 3K tiny ship instances.
</div>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/P2Net.png"><img src='image/P2Net.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<b>Contrastive Learning for Fine-grained Ship Classification in Remote Sensing Images</b><br>
<i>IEEE Transactions on Geoscience and Remote Sensing (TGRS), 2022</i><br>
Jianqi Chen, <b>Keyan Chen</b>, Hao Chen, Wenyuan Li, Zhengxia Zou, and Zhenwei Shi<br>
[<a href="http://levir.buaa.edu.cn/publications/CLFSC.pdf">PDF</a>] [<a href="https://github.com/WindVChen/Push-and-Pull-Network">Github</a>]<br>
<div style="text-align: justify">
We propose an asynchronous contrastive learning-based method for effective fine-grained ship classification, which refers to as "Push-and-Pull Network (P2Net)", includes a "push-out stage" and a "pull-in stage", where the first stage forces all the instances to be de-correlated and then the second one groups them into each subclass.
</div>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/GeoKR.png"><img src='image/GeoKR.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<b>Geographical Knowledge-Driven Representation Learning for Remote Sensing Images</b><br>
<i>IEEE Transactions on Geoscience and Remote Sensing (TGRS), 2021</i><br>
Wenyuan Li, <b>Keyan Chen</b>, Hao Chen and Zhenwei Shi<br>
[<a href="http://levir.buaa.edu.cn/publications/Geographical_Knowledge-Driven.pdf">PDF</a>] [<a href="https://github.com/flyakon/Geographical-Knowledge-driven-Representaion-Learning">Github</a>]<br>
<div style="text-align: justify">
 We propose a Geographical Knowledge-driven Representation learning method for remote sensing images (GeoKR), improving network performance and reduce the demand for annotated data. The global land cover products and geographical location associated with each remote sensing image are regarded as geographical knowledge to provide supervision for representation learning and network pre-training.
</div>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/STT.png"><img src='image/STT.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<b>Building Extraction from Remote Sensing Images with Sparse Token Transformers</b><br>
<i>Remote Sensing, 2021</i><br>
<b>Keyan Chen</b>, Zhengxia Zou and Zhenwei Shi<br>
[<a href="https://www.mdpi.com/2072-4292/13/21/4441">PDF</a>] [<a href="https://github.com/KyanChen/STT">Github</a>] [<a href="https://kyanchen.github.io/STT/">Page</a>] [<a href="https://huggingface.co/spaces/KyanChen/BuildingExtraction">Demo</a>]<br>
<div style="text-align: justify">
We propose STT to explore the potential of using transformers for efficient building extraction. STT conducts an efficient dual-pathway transformer that learns the global semantic information in both their spatial and channel dimensions and achieves state-of-the-art accuracy on two building extraction benchmarks.
</div>

</div>
</div>
