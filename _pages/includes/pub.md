
# üìù Publications

My full paper list can be found at my [<a href="https://scholar.google.com/citations?user=Mr7Dx6gAAAAJ&hl=zh-CN">Google Scholar</a>].


<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/Dexterous_manipulation.png"><img src='image/Dexterous_manipulation.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<b>Design and experiment of on-orbit assembly ground simulation robot</b><br>
<i>The 5th International Conference on Mechanical, Electric, and Industrial Engineering (MEIE 2022)</i><br>
<b>Chuanke Pang</b>, and Rui Zhong<br>
[<a href="https://iopscience.iop.org/article/10.1088/1742-6596/2369/1/012071/meta">Page</a>][<a href="https://github.com/chuankepang/dual_arm_collaborative_robot">Github</a>]<br>
<div style="text-align: justify">
we propose a precise-dexterous assembly framework based on closed-loop control, integrating the functional modules of position estimation, collision detection, trajectory planning, motion control, etc., for the self-designed reflecting space telescope model. 
</div>
</div>

</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/Dual-Arm Collaborative Robot.png"><img src='image/Dual-Arm Collaborative Robot.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<b>High Precision Assembly with Dual-Arm Collaborative Robot Based on the Fusion of Vision and Force-Haptics</b><br>
<i>International Conference on Space and Vehicle Control (ICSCACC) 2022</i><br>
<b>Chuanke Pang</b>, and Rui Zhong<br>
<div style="text-align: justify">
We propose a ROS-based robot system for simulated on-orbit assembly, in the context of the project "Development of Adaptive Control and Operating Modules for Precision Manipulation". 
</div>
</div>

</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/Pose_Observer.png"><img src='image/Pose_Observer.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<b>A New Potential Functions-Based Pose Observer Design Method for Cooperative Target</b><br>
<i>Patent</i><br>
Rui Zhong, Hangbiao Zhu, <b>Chuanke Pang</b>, Yinghong Jia, Jun Zhang, and Kai Wang<br>
<!-- [<a href="https://arxiv.org/abs/2301.09506">Arxiv</a>] [<a href="https://github.com/KyanChen/OvarNet">Github</a>] [<a href="https://kyanchen.github.io/OvarNet/">Page</a>]<br> -->
<div style="text-align: justify">
We develop a cooperative target pose observer based on a new potential function, which is used for real-time position and attitude estimation of spatial cooperative targets.
</div>
</div>

</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/ODSurvey.png"><img src='image/ODSurvey.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<b>Object Detection in 20 Years: A Survey</b><br>
<i>Proceedings of the IEEE (P IEEE), 2023</i><br>
Zhengxia Zou, <b>Keyan Chen</b>, Zhenwei Shi, Yuhong Guo and Jieping Ye<br>
[<a href="http://levir.buaa.edu.cn/publications/od_survey.pdf">PDF</a>] [<a href="https://github.com/KyanChen/ODSurvey">Github</a>] [<a href="https://kyanchen.github.io/ODSurvey/">Page</a>]<br>
<div style="text-align: justify">
This paper extensively reviews the fast-moving research field in the light of technical evolution, spanning over a quarter-century's time (from the 1990s to 2022). A number of topics have been covered, including the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speed-up techniques, and the recent state-of-the-art detection methods.
</div>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/RASNet.png"><img src='image/RASNet.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<b>Resolution-agnostic Remote Sensing Scene Classification with Implicit Neural Representations</b><br>
<i>IEEE Geoscience and Remote Sensing Letters (GRSL), 2022</i><br>
<b>Keyan Chen</b>, Wenyuan Li, Jianqi Chen, Zhengxia Zou and Zhenwei Shi<br>
[<a href="http://levir.buaa.edu.cn/publications/RASNet.pdf">PDF</a>] [<a href="https://github.com/KyanChen/RASNet">Github</a>] [<a href="https://kyanchen.github.io/RASNet/">Page</a>]<br>
<div style="text-align: justify">
We propose a novel scene classification method with scale and resolution adaptation ability. Unlike previous CNNbased methods that make predictions based on rasterized image inputs, the proposed method converts the images as continuous functions with INRs optimization and then performs classification within the function space.
</div>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/GeCo.png"><img src='image/GeCo.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<b>Geographical Supervision Correction for Remote Sensing Representation Learning</b><br>
<i>IEEE Transactions on Geoscience and Remote Sensing (TGRS), 2022</i><br>
Wenyuan Li, <b>Keyan Chen</b>, and Zhenwei Shi<br>
[<a href="http://levir.buaa.edu.cn/publications/FINAL_VERSION.pdf">PDF</a>]<br>
<div style="text-align: justify">
We propose a Geographical supervision Correction method (GeCo) for remote sensing representation learning. Deviated geographical supervision generated by GLC products can be corrected adaptively using the correction matrix during network pre-training and joint optimization process is designed to simultaneously update the correction matrix and network parameters.
</div>

</div>
</div>



<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/DRENet.png"><img src='image/DRENet.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<b>A Degraded Reconstruction Enhancement-based Method for Tiny Ship Detection in Remote Sensing Images with A New Large-scale Dataset</b><br>
<i>IEEE Transactions on Geoscience and Remote Sensing (TGRS), 2022</i><br>
Jianqi Chen, <b>Keyan Chen</b>, Hao Chen, Zhengxia Zou and Zhenwei Shi<br>
[<a href="http://levir.buaa.edu.cn/publications/DRENet.pdf">PDF</a>] [<a href="https://github.com/WindVChen/DRENet">Github</a>] [<a href="https://github.com/windvchen/levir-ship">Dataset</a>]<br>
<div style="text-align: justify">
We propose a tiny ship detection method namely, Degraded Reconstruction Enhancement Network (DRENet), for medium-resolution remote sensing images, and introduce Levir-Ship, which contains 3876 GF-1/GF-6 multi-spectral images and over 3K tiny ship instances.
</div>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/P2Net.png"><img src='image/P2Net.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<b>Contrastive Learning for Fine-grained Ship Classification in Remote Sensing Images</b><br>
<i>IEEE Transactions on Geoscience and Remote Sensing (TGRS), 2022</i><br>
Jianqi Chen, <b>Keyan Chen</b>, Hao Chen, Wenyuan Li, Zhengxia Zou, and Zhenwei Shi<br>
[<a href="http://levir.buaa.edu.cn/publications/CLFSC.pdf">PDF</a>] [<a href="https://github.com/WindVChen/Push-and-Pull-Network">Github</a>]<br>
<div style="text-align: justify">
We propose an asynchronous contrastive learning-based method for effective fine-grained ship classification, which refers to as "Push-and-Pull Network (P2Net)", includes a "push-out stage" and a "pull-in stage", where the first stage forces all the instances to be de-correlated and then the second one groups them into each subclass.
</div>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/GeoKR.png"><img src='image/GeoKR.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<b>Geographical Knowledge-Driven Representation Learning for Remote Sensing Images</b><br>
<i>IEEE Transactions on Geoscience and Remote Sensing (TGRS), 2021</i><br>
Wenyuan Li, <b>Keyan Chen</b>, Hao Chen and Zhenwei Shi<br>
[<a href="http://levir.buaa.edu.cn/publications/Geographical_Knowledge-Driven.pdf">PDF</a>] [<a href="https://github.com/flyakon/Geographical-Knowledge-driven-Representaion-Learning">Github</a>]<br>
<div style="text-align: justify">
 We propose a Geographical Knowledge-driven Representation learning method for remote sensing images (GeoKR), improving network performance and reduce the demand for annotated data. The global land cover products and geographical location associated with each remote sensing image are regarded as geographical knowledge to provide supervision for representation learning and network pre-training.
</div>

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><a href="image/STT.png"><img src='image/STT.png' alt="sym" width="100%"></a></div></div>
<div class='paper-box-text' markdown="1">

<b>Building Extraction from Remote Sensing Images with Sparse Token Transformers</b><br>
<i>Remote Sensing, 2021</i><br>
<b>Keyan Chen</b>, Zhengxia Zou and Zhenwei Shi<br>
[<a href="https://www.mdpi.com/2072-4292/13/21/4441">PDF</a>] [<a href="https://github.com/KyanChen/STT">Github</a>] [<a href="https://kyanchen.github.io/STT/">Page</a>] [<a href="https://huggingface.co/spaces/KyanChen/BuildingExtraction">Demo</a>]<br>
<div style="text-align: justify">
We propose STT to explore the potential of using transformers for efficient building extraction. STT conducts an efficient dual-pathway transformer that learns the global semantic information in both their spatial and channel dimensions and achieves state-of-the-art accuracy on two building extraction benchmarks.
</div>

</div>
</div>
